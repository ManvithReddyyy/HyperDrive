import type { Express } from "express";
import { createServer, type Server } from "http";
import { WebSocketServer, WebSocket } from "ws";
import { storage } from "./storage";
import { createJobSchema, inferenceRequestSchema } from "@shared/schema";
import { z } from "zod";
import { supabase } from "./supabase";
import { addJobConnection, removeJobConnection } from "./broadcast";

export async function registerRoutes(
  httpServer: Server,
  app: Express
): Promise<Server> {
  const wss = new WebSocketServer({ server: httpServer, path: "/ws" });

  wss.on("connection", (ws, req) => {
    const url = new URL(req.url || "", `http://${req.headers.host}`);
    const jobId = url.searchParams.get("jobId");

    if (jobId) {
      console.log(`WebSocket connected for job ${jobId}`);
      addJobConnection(jobId, ws);

      // Send current job state immediately
      storage.getJob(jobId).then(job => {
        if (job && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: "job_update", job }));
        }
      });

      ws.on("close", () => {
        removeJobConnection(jobId, ws);
      });
    }

    ws.on("error", console.error);
  });

  app.post("/api/jobs", async (req, res) => {
    try {
      const data = createJobSchema.parse(req.body);

      // Get user ID from Supabase token
      const authHeader = req.headers.authorization;
      let userId = "00000000-0000-0000-0000-000000000000";

      if (authHeader && authHeader.startsWith("Bearer ")) {
        const token = authHeader.substring(7);
        const { data: { user }, error } = await supabase.auth.getUser(token);
        if (user && !error) {
          userId = user.id;

          // Ensure user exists in public.users table
          const { data: existingUser } = await supabase
            .from('users')
            .select('id')
            .eq('id', userId)
            .single();

          if (!existingUser) {
            // Create user profile if it doesn't exist
            await supabase
              .from('users')
              .insert({
                id: userId,
                username: user.email || 'user',
                email: user.email,
              });
          }
        }
      }

      console.log("Creating job with userId:", userId);
      const job = await storage.createJob(data, userId);
      res.json(job);
    } catch (error) {
      if (error instanceof z.ZodError) {
        res.status(400).json({ error: "Invalid request", details: error.errors });
      } else {
        console.error("Error creating job:", error);
        res.status(500).json({ error: "Failed to create job" });
      }
    }
  });

  // Update user profile
  app.patch("/api/user/profile", async (req, res) => {
    try {
      const authHeader = req.headers.authorization;

      if (!authHeader || !authHeader.startsWith("Bearer ")) {
        res.status(401).json({ error: "Unauthorized" });
        return;
      }

      const token = authHeader.substring(7);
      const { data: { user }, error: authError } = await supabase.auth.getUser(token);

      if (!user || authError) {
        res.status(401).json({ error: "Invalid token" });
        return;
      }

      const { displayName } = req.body;

      if (!displayName || typeof displayName !== "string") {
        res.status(400).json({ error: "Display name is required" });
        return;
      }

      // Update username in public.users table
      const { error: updateError } = await supabase
        .from("users")
        .update({ username: displayName.trim() })
        .eq("id", user.id);

      if (updateError) {
        console.error("Error updating user profile:", updateError);
        res.status(500).json({ error: "Failed to update profile" });
        return;
      }

      res.json({ success: true, displayName: displayName.trim() });
    } catch (error) {
      console.error("Error updating profile:", error);
      res.status(500).json({ error: "Failed to update profile" });
    }
  });

  app.get("/api/jobs", async (req, res) => {
    try {
      const userId = (req as any).user?.id;
      const jobs = await storage.getAllJobs(userId);
      res.json(jobs);
    } catch (error) {
      console.error("Error fetching jobs:", error);
      res.status(500).json({ error: "Failed to fetch jobs" });
    }
  });

  app.get("/api/jobs/:id", async (req, res) => {
    try {
      const job = await storage.getJob(req.params.id);
      if (!job) {
        res.status(404).json({ error: "Job not found" });
        return;
      }
      res.json(job);
    } catch (error) {
      console.error("Error fetching job:", error);
      res.status(500).json({ error: "Failed to fetch job" });
    }
  });

  app.post("/api/inference/compare", async (req, res) => {
    try {
      const data = inferenceRequestSchema.parse(req.body);

      const originalLatency = 80 + Math.floor(Math.random() * 80);
      const optimizedLatency = 25 + Math.floor(Math.random() * 35);

      const generateOutput = (prompt: string, isOptimized: boolean): string => {
        const baseResponses = [
          `Based on the input "${prompt.slice(0, 50)}${prompt.length > 50 ? "..." : ""}", the model predicts:`,
          `Analysis complete for: "${prompt.slice(0, 30)}${prompt.length > 30 ? "..." : ""}"`,
          `Processing input: "${prompt.slice(0, 40)}${prompt.length > 40 ? "..." : ""}"`,
        ];

        const base = baseResponses[Math.floor(Math.random() * baseResponses.length)];
        const detail = isOptimized
          ? "\n\nThe optimized model maintains high accuracy while achieving faster inference through quantization and graph optimizations. Confidence score: 0.94"
          : "\n\nThe original model provides baseline predictions with standard precision. Confidence score: 0.92";

        return base + detail;
      };

      await new Promise(resolve => setTimeout(resolve, originalLatency + optimizedLatency));

      res.json({
        original: {
          output: generateOutput(data.prompt, false),
          latency: originalLatency,
        },
        optimized: {
          output: generateOutput(data.prompt, true),
          latency: optimizedLatency,
        },
      });
    } catch (error) {
      if (error instanceof z.ZodError) {
        res.status(400).json({ error: "Invalid request", details: error.errors });
      } else {
        console.error("Error running inference:", error);
        res.status(500).json({ error: "Failed to run inference comparison" });
      }
    }
  });

  app.get("/api/deploy/:id", async (req, res) => {
    try {
      const code = await storage.getDeploymentCode(req.params.id);
      if (!code) {
        res.status(404).json({ error: "Deployment code not available" });
        return;
      }
      res.json(code);
    } catch (error) {
      console.error("Error fetching deployment code:", error);
      res.status(500).json({ error: "Failed to fetch deployment code" });
    }
  });

  // Sensitivity Analysis endpoint
  app.get("/api/jobs/:id/sensitivity", async (req, res) => {
    try {
      const jobId = req.params.id;
      // Generate deterministic pseudo-random data based on jobId
      const seed = jobId.split("").reduce((acc, c) => acc + c.charCodeAt(0), 0);
      const rng = (offset: number) => {
        const x = Math.sin(seed + offset) * 10000;
        return x - Math.floor(x);
      };

      const layers = [];
      const numLayers = 6 + Math.floor(rng(1) * 6);
      const layerTypes = ["Conv", "Attn", "Dense", "LayerNorm"];

      for (let i = 1; i <= numLayers; i++) {
        const layerType = layerTypes[Math.floor(rng(i * 2) * layerTypes.length)];
        const name = `${layerType}_${i}`;
        const base = rng(i * 3) * 0.08;
        const err =
          rng(i * 4) < 0.08
            ? Math.round((base + rng(i * 5) * 0.4) * 1000) / 1000
            : Math.round(base * 1000) / 1000;
        layers.push({ layer: name, error: err });
      }
      res.json(layers);
    } catch (error) {
      console.error("Error fetching sensitivity data:", error);
      res.status(500).json({ error: "Failed to fetch sensitivity data" });
    }
  });

  // Architecture Graph endpoint
  app.get("/api/jobs/:id/graph", async (req, res) => {
    try {
      const jobId = req.params.id;
      const seed = jobId.split("").reduce((acc, c) => acc + c.charCodeAt(0), 0) * 7;
      const rng = (offset: number) => {
        const x = Math.sin(seed + offset) * 10000;
        return x - Math.floor(x);
      };

      const nodes = [];
      const edges = [];
      const layerNames = ["Input", "Conv_1", "Conv_2", "Pool", "Attn_1", "Dense", "Output"];
      const xStart = 50;
      const yStart = 100;
      const xGap = 220;

      for (let idx = 0; idx < layerNames.length; idx++) {
        const lname = layerNames[idx];
        const nodeId = `n${idx}`;
        const x = xStart + idx * xGap;
        const y = yStart + (idx % 2) * 30;
        const fused = lname.includes("Conv") && rng(idx * 100) < 0.25;

        nodes.push({
          id: nodeId,
          data: { label: lname, fused },
          position: { x, y },
          style: {},
        });

        if (idx > 0) {
          edges.push({
            id: `e${idx - 1}`,
            source: `n${idx - 1}`,
            target: nodeId,
            animated: false,
          });
        }
      }

      res.json({ nodes, edges });
    } catch (error) {
      console.error("Error fetching graph data:", error);
      res.status(500).json({ error: "Failed to fetch graph data" });
    }
  });

  // Hardware Matrix endpoint
  app.get("/api/hardware-matrix", async (req, res) => {
    try {
      const options = [
        { name: "cpu-small", cost_per_hour: 0.1, throughput_tokens_s: 20 },
        { name: "cpu-large", cost_per_hour: 0.5, throughput_tokens_s: 60 },
        { name: "gpu-v100", cost_per_hour: 2.4, throughput_tokens_s: 400 },
        { name: "gpu-a10", cost_per_hour: 1.8, throughput_tokens_s: 320 },
        { name: "tpu-small", cost_per_hour: 3.2, throughput_tokens_s: 600 },
        { name: "edge-rt", cost_per_hour: 0.75, throughput_tokens_s: 150 },
      ];
      res.json(options);
    } catch (error) {
      console.error("Error fetching hardware matrix:", error);
      res.status(500).json({ error: "Failed to fetch hardware matrix" });
    }
  });

  // Upload endpoint with calibration file support
  app.post("/api/upload", async (req, res) => {
    try {
      // Simple mock response - in production this would handle multipart form data
      const file = req.body.file || "model.pt";
      const calibrationFile = req.body.calibration_file || null;

      res.json({
        status: "ok",
        file: { filename: file, content_type: "application/octet-stream" },
        calibration_file: calibrationFile
          ? { filename: calibrationFile, content_type: "application/jsonl" }
          : null,
        metadata: req.body.metadata || null,
      });
    } catch (error) {
      console.error("Error uploading file:", error);
      res.status(500).json({ error: "Failed to upload file" });
    }
  });

  // ============ ENTERPRISE FEATURES ============

  // Autopilot / Neural Architecture Search - returns optimization trials
  app.get("/api/jobs/:id/autopilot", async (req, res) => {
    try {
      const jobId = req.params.id;
      const job = await storage.getJob(jobId);

      if (!job) {
        res.status(404).json({ error: "Job not found" });
        return;
      }

      // Generate deterministic trials based on job config
      const seed = jobId.split("").reduce((acc, c) => acc + c.charCodeAt(0), 0);
      const rng = (offset: number) => {
        const x = Math.sin(seed + offset) * 10000;
        return x - Math.floor(x);
      };

      const strategies = [
        { name: "INT8 Static (Per-Tensor)", quantization: "INT8", mode: "static", granularity: "per-tensor" },
        { name: "INT8 Static (Per-Channel)", quantization: "INT8", mode: "static", granularity: "per-channel" },
        { name: "INT8 Dynamic", quantization: "INT8", mode: "dynamic", granularity: "per-tensor" },
        { name: "FP16 (Half Precision)", quantization: "FP16", mode: "native", granularity: "full" },
        { name: "Mixed INT8/FP16", quantization: "Mixed", mode: "hybrid", granularity: "per-layer" },
        { name: "INT4 AWQ", quantization: "INT4", mode: "awq", granularity: "per-group" },
      ];

      const trials = strategies.map((strategy, idx) => {
        const baseLatency = 15 + rng(idx * 10) * 25;
        const latency = Math.round(baseLatency * 10) / 10;
        const accuracyLoss = Math.round((rng(idx * 20) * 2 - 0.2) * 100) / 100;
        const baseSize = 45 + rng(idx * 30) * 80;
        const size = Math.round(baseSize);

        return {
          id: idx + 1,
          strategy: strategy.name,
          latency: `${latency}ms`,
          latencyValue: latency,
          accuracyLoss: `${accuracyLoss > 0 ? '-' : '+'}${Math.abs(accuracyLoss)}%`,
          accuracyLossValue: accuracyLoss,
          size: `${size}MB`,
          sizeValue: size,
          recommended: false,
        };
      });

      // Find the best trial (lowest latency with acceptable accuracy loss)
      const validTrials = trials.filter(t => t.accuracyLossValue < 1.5);
      if (validTrials.length > 0) {
        const best = validTrials.reduce((a, b) =>
          a.latencyValue < b.latencyValue ? a : b
        );
        best.recommended = true;
      }

      res.json(trials);
    } catch (error) {
      console.error("Error fetching autopilot data:", error);
      res.status(500).json({ error: "Failed to fetch autopilot data" });
    }
  });

  // Drift Simulator - simulates accuracy degradation under data drift
  app.post("/api/simulation/drift", async (req, res) => {
    try {
      const { driftLevel, jobId } = req.body;

      if (typeof driftLevel !== "number" || driftLevel < 0 || driftLevel > 100) {
        res.status(400).json({ error: "Invalid drift level (0-100)" });
        return;
      }

      // Simulate accuracy curve with drift
      // At 0% drift, accuracy is ~98%
      // Accuracy degrades non-linearly as drift increases
      const seed = jobId ? jobId.split("").reduce((acc: number, c: string) => acc + c.charCodeAt(0), 0) : 42;
      const baseAccuracy = 98;
      const noise = Math.sin(seed + driftLevel) * 2;

      // Non-linear degradation curve
      const degradation = Math.pow(driftLevel / 100, 1.5) * 35 + (driftLevel / 100) * 15;
      const accuracy = Math.max(45, Math.min(100, baseAccuracy - degradation + noise));

      // Generate full curve data points
      const curveData = [];
      for (let d = 0; d <= 100; d += 5) {
        const deg = Math.pow(d / 100, 1.5) * 35 + (d / 100) * 15;
        const n = Math.sin(seed + d) * 1.5;
        curveData.push({
          drift: d,
          accuracy: Math.round((Math.max(45, baseAccuracy - deg + n)) * 10) / 10,
        });
      }

      res.json({
        currentDrift: driftLevel,
        currentAccuracy: Math.round(accuracy * 10) / 10,
        breakingPoint: curveData.find(p => p.accuracy < 80)?.drift || 100,
        curveData,
      });
    } catch (error) {
      console.error("Error running drift simulation:", error);
      res.status(500).json({ error: "Failed to run drift simulation" });
    }
  });

  // Model Nutrition Label - AI Bill of Materials
  app.get("/api/models/:id/nutrition", async (req, res) => {
    try {
      const jobId = req.params.id;
      const job = await storage.getJob(jobId);

      if (!job) {
        res.status(404).json({ error: "Model not found" });
        return;
      }

      // Generate nutrition label data
      const seed = jobId.split("").reduce((acc, c) => acc + c.charCodeAt(0), 0);
      const rng = (offset: number) => {
        const x = Math.sin(seed + offset) * 10000;
        return x - Math.floor(x);
      };

      const architectures = ["Transformer", "CNN", "RNN", "Hybrid", "MLP-Mixer"];
      const licenses = ["Apache 2.0", "MIT", "BSD-3", "GPL-3.0", "Proprietary"];
      const datasets = ["CommonCrawl", "ImageNet", "LAION", "Pile", "Custom"];

      const params = Math.round((0.5 + rng(1) * 13) * 10) / 10;
      const co2 = Math.round((5 + rng(2) * 50) * 10) / 10;
      const energy = Math.round((2 + rng(3) * 20) * 10) / 10;
      const tokens = Math.round(100 + rng(4) * 900);

      res.json({
        modelName: job.fileName.replace(/\.(pt|onnx|safetensors)$/, ""),
        architecture: architectures[Math.floor(rng(5) * architectures.length)],
        parameters: params >= 1 ? `${params}B` : `${Math.round(params * 1000)}M`,
        license: licenses[Math.floor(rng(6) * licenses.length)],
        trainingData: datasets[Math.floor(rng(7) * datasets.length)],
        co2Emitted: `${co2}kg COâ‚‚`,
        energyUsed: `${energy}kWh`,
        tokensProcessed: `${tokens}B`,
        quantization: job.config.quantization,
        targetDevice: job.config.targetDevice,
        optimizationDate: job.completedAt || job.createdAt,
        originalLatency: job.originalLatency ? `${job.originalLatency}ms` : "N/A",
        optimizedLatency: job.optimizedLatency ? `${job.optimizedLatency}ms` : "N/A",
        sizeReduction: job.sizeReduction ? `${job.sizeReduction}%` : "N/A",
      });
    } catch (error) {
      console.error("Error fetching nutrition label:", error);
      res.status(500).json({ error: "Failed to fetch nutrition label" });
    }
  });

  // Memory Stress Test - simulates VRAM/RAM usage under different batch sizes
  app.post("/api/simulation/memory", async (req, res) => {
    try {
      const { batchSize, jobId } = req.body;

      if (typeof batchSize !== "number" || batchSize < 1 || batchSize > 128) {
        res.status(400).json({ error: "Invalid batch size (1-128)" });
        return;
      }

      const seed = jobId ? jobId.split("").reduce((acc: number, c: string) => acc + c.charCodeAt(0), 0) : 42;

      // Base memory footprint (model size)
      const baseVRAM = 2.5 + (seed % 10) * 0.3; // 2.5-5.5 GB base
      const baseRAM = 4 + (seed % 8) * 0.5; // 4-8 GB base

      // Memory scales with batch size (not linearly due to optimization)
      const vramMultiplier = 1 + Math.log2(batchSize) * 0.4;
      const ramMultiplier = 1 + Math.log2(batchSize) * 0.25;

      const currentVRAM = Math.round(baseVRAM * vramMultiplier * 100) / 100;
      const currentRAM = Math.round(baseRAM * ramMultiplier * 100) / 100;

      // Peak memory (with spikes during forward pass)
      const peakVRAM = Math.round(currentVRAM * 1.3 * 100) / 100;
      const peakRAM = Math.round(currentRAM * 1.15 * 100) / 100;

      // VRAM limit (simulated GPU memory)
      const vramLimit = 24; // RTX 4090 / A10
      const utilizationPercent = Math.round((peakVRAM / vramLimit) * 100);
      const isOOM = peakVRAM > vramLimit;

      // Generate curve data
      const curveData = [];
      for (let bs = 1; bs <= 128; bs *= 2) {
        const vm = baseVRAM * (1 + Math.log2(bs) * 0.4);
        const rm = baseRAM * (1 + Math.log2(bs) * 0.25);
        curveData.push({
          batchSize: bs,
          vram: Math.round(vm * 100) / 100,
          ram: Math.round(rm * 100) / 100,
          peakVram: Math.round(vm * 1.3 * 100) / 100,
        });
      }

      res.json({
        currentBatchSize: batchSize,
        vram: currentVRAM,
        ram: currentRAM,
        peakVram: peakVRAM,
        peakRam: peakRAM,
        vramLimit,
        utilizationPercent,
        isOOM,
        maxSafeBatch: curveData.filter(d => d.peakVram <= vramLimit).pop()?.batchSize || 1,
        curveData,
      });
    } catch (error) {
      console.error("Error running memory simulation:", error);
      res.status(500).json({ error: "Failed to run memory simulation" });
    }
  });

  // Throughput Benchmark - measures tokens/images per second at different batch sizes
  app.post("/api/simulation/throughput", async (req, res) => {
    try {
      const { batchSize, jobId } = req.body;

      if (typeof batchSize !== "number" || batchSize < 1 || batchSize > 128) {
        res.status(400).json({ error: "Invalid batch size (1-128)" });
        return;
      }

      const seed = jobId ? jobId.split("").reduce((acc: number, c: string) => acc + c.charCodeAt(0), 0) : 42;

      // Base throughput (single sample)
      const baseThroughput = 50 + (seed % 50); // 50-100 samples/sec base

      // Throughput increases with batch size but with diminishing returns
      // Also drops slightly at very high batch sizes due to memory pressure
      const efficiency = batchSize <= 32
        ? 0.95 - (batchSize - 1) * 0.005
        : 0.8 - (batchSize - 32) * 0.003;

      const currentThroughput = Math.round(baseThroughput * batchSize * efficiency);
      const latencyMs = Math.round((batchSize / currentThroughput) * 1000 * 10) / 10;

      // Calculate optimal batch size
      let optimalBatch = 1;
      let maxThroughput = 0;
      for (let bs = 1; bs <= 128; bs *= 2) {
        const eff = bs <= 32 ? 0.95 - (bs - 1) * 0.005 : 0.8 - (bs - 32) * 0.003;
        const tp = baseThroughput * bs * eff;
        if (tp > maxThroughput) {
          maxThroughput = tp;
          optimalBatch = bs;
        }
      }

      // Generate curve data
      const curveData = [];
      for (let bs = 1; bs <= 128; bs *= 2) {
        const eff = bs <= 32 ? 0.95 - (bs - 1) * 0.005 : 0.8 - (bs - 32) * 0.003;
        const tp = Math.round(baseThroughput * bs * eff);
        const lat = Math.round((bs / tp) * 1000 * 10) / 10;
        curveData.push({
          batchSize: bs,
          throughput: tp,
          latency: lat,
          efficiency: Math.round(eff * 100),
        });
      }

      res.json({
        currentBatchSize: batchSize,
        throughput: currentThroughput,
        latencyMs,
        unit: "samples/sec",
        optimalBatchSize: optimalBatch,
        maxThroughput: Math.round(maxThroughput),
        curveData,
      });
    } catch (error) {
      console.error("Error running throughput simulation:", error);
      res.status(500).json({ error: "Failed to run throughput simulation" });
    }
  });

  // ============ PREMIUM ENTERPRISE FEATURES ============

  // Model Comparison - Compare two jobs side by side
  app.get("/api/compare/:jobId1/:jobId2", async (req, res) => {
    try {
      const { jobId1, jobId2 } = req.params;

      const job1 = await storage.getJob(jobId1);
      const job2 = await storage.getJob(jobId2);

      if (!job1 || !job2) {
        return res.status(404).json({ error: "One or both jobs not found" });
      }

      // Calculate improvement percentages
      const sizeImprovement = (job2.sizeReduction || 0) - (job1.sizeReduction || 0);
      const latencyImprovement = job1.optimizedLatency && job2.optimizedLatency
        ? ((job1.optimizedLatency - job2.optimizedLatency) / job1.optimizedLatency * 100)
        : 0;

      res.json({
        job1: {
          id: job1.id,
          fileName: job1.fileName,
          originalSize: job1.fileSize,
          optimizedSize: Math.round(job1.fileSize * (1 - (job1.sizeReduction || 40) / 100)),
          sizeReduction: job1.sizeReduction || 40,
          originalLatency: job1.originalLatency,
          optimizedLatency: job1.optimizedLatency,
          config: job1.config,
          status: job1.status,
          createdAt: job1.createdAt,
        },
        job2: {
          id: job2.id,
          fileName: job2.fileName,
          originalSize: job2.fileSize,
          optimizedSize: Math.round(job2.fileSize * (1 - (job2.sizeReduction || 40) / 100)),
          sizeReduction: job2.sizeReduction || 40,
          originalLatency: job2.originalLatency,
          optimizedLatency: job2.optimizedLatency,
          config: job2.config,
          status: job2.status,
          createdAt: job2.createdAt,
        },
        comparison: {
          sizeImprovement: Math.round(sizeImprovement * 10) / 10,
          latencyImprovement: Math.round(latencyImprovement * 10) / 10,
          winner: sizeImprovement > 0 ? "job2" : sizeImprovement < 0 ? "job1" : "tie",
        },
      });
    } catch (error) {
      console.error("Error comparing jobs:", error);
      res.status(500).json({ error: "Failed to compare jobs" });
    }
  });

  // Insights Dashboard - Aggregate statistics
  app.get("/api/insights", async (req, res) => {
    try {
      const userId = (req as any).user?.id;
      const jobs = await storage.getAllJobs(userId);
      const completedJobs = jobs.filter((j: any) => j.status === "completed");

      // Calculate real aggregates from actual job data
      const totalModelsOptimized = completedJobs.length;

      // Helper to estimate model size based on filename if fileSize is 0
      const getEstimatedSize = (job: any): number => {
        if (job.fileSize && job.fileSize > 0) return job.fileSize;
        // Estimate based on file extension and common model sizes
        const fileName = (job.fileName || "").toLowerCase();
        if (fileName.includes("llama") || fileName.includes("gpt")) return 7 * 1024 * 1024 * 1024; // 7GB
        if (fileName.includes("bert")) return 440 * 1024 * 1024; // 440MB
        if (fileName.includes("resnet") || fileName.includes("vgg")) return 550 * 1024 * 1024; // 550MB
        if (fileName.includes("yolo") || fileName.includes("detection")) return 250 * 1024 * 1024; // 250MB
        if (fileName.endsWith(".pt") || fileName.endsWith(".pth")) return 200 * 1024 * 1024; // 200MB default PyTorch
        if (fileName.endsWith(".onnx")) return 150 * 1024 * 1024; // 150MB default ONNX
        if (fileName.endsWith(".tflite")) return 50 * 1024 * 1024; // 50MB TFLite
        return 100 * 1024 * 1024; // 100MB default
      };

      // Calculate sizes with estimation
      const totalOriginalSize = completedJobs.reduce((sum: number, j: any) => sum + getEstimatedSize(j), 0);
      const totalOptimizedSize = completedJobs.reduce((sum: number, j: any) => {
        const original = getEstimatedSize(j);
        const reduction = j.sizeReduction || 0;
        return sum + Math.round(original * (1 - reduction / 100));
      }, 0);
      const totalSizeSaved = totalOriginalSize - totalOptimizedSize;

      // Real average size reduction from actual jobs
      const avgSizeReduction = completedJobs.length > 0
        ? completedJobs.reduce((sum: number, j: any) => sum + (j.sizeReduction || 0), 0) / completedJobs.length
        : 0;

      // Real average latency reduction from actual jobs
      const jobsWithLatency = completedJobs.filter((j: any) => j.originalLatency && j.optimizedLatency);
      const avgLatencyReduction = jobsWithLatency.length > 0
        ? jobsWithLatency.reduce((sum: number, j: any) => {
          return sum + ((j.originalLatency - j.optimizedLatency) / j.originalLatency * 100);
        }, 0) / jobsWithLatency.length
        : 0;

      // Quantization breakdown from actual job configs
      const quantizationBreakdown: Record<string, number> = {};
      completedJobs.forEach((j: any) => {
        const q = j.config?.quantization || "Unknown";
        quantizationBreakdown[q] = (quantizationBreakdown[q] || 0) + 1;
      });

      // Target device breakdown from actual job configs
      const deviceBreakdown: Record<string, number> = {};
      completedJobs.forEach((j: any) => {
        const d = j.config?.targetDevice || "Unknown";
        deviceBreakdown[d] = (deviceBreakdown[d] || 0) + 1;
      });

      // Monthly trend from actual job dates
      const monthlyTrend = [];
      const now = new Date();
      for (let i = 5; i >= 0; i--) {
        const month = new Date(now.getFullYear(), now.getMonth() - i, 1);
        const nextMonth = new Date(now.getFullYear(), now.getMonth() - i + 1, 1);
        const monthJobs = completedJobs.filter((j: any) => {
          const jobDate = j.createdAt ? new Date(j.createdAt) : null;
          return jobDate && jobDate >= month && jobDate < nextMonth;
        });

        // Calculate actual size saved for this month
        const monthSizeSaved = monthJobs.reduce((sum: number, j: any) => {
          const original = getEstimatedSize(j);
          const reduction = j.sizeReduction || 0;
          return sum + Math.round(original * reduction / 100);
        }, 0);

        monthlyTrend.push({
          month: month.toLocaleDateString("en-US", { month: "short", year: "2-digit" }),
          models: monthJobs.length,
          sizeSaved: monthSizeSaved,
        });
      }

      // Recent activity from actual jobs
      const recentJobs = jobs.slice(0, 5).map((j: any) => ({
        id: j.id,
        fileName: j.fileName,
        status: j.status,
        sizeReduction: j.sizeReduction || 0,
        createdAt: j.createdAt,
      }));

      // Calculate real streak days (consecutive days with completed jobs)
      let streakDays = 0;
      if (completedJobs.length > 0) {
        const sortedDates = completedJobs
          .filter((j: any) => j.completedAt)
          .map((j: any) => new Date(j.completedAt).toDateString())
          .filter((date: string, index: number, self: string[]) => self.indexOf(date) === index)
          .sort((a: string, b: string) => new Date(b).getTime() - new Date(a).getTime());

        const today = new Date().toDateString();
        const yesterday = new Date(Date.now() - 86400000).toDateString();

        if (sortedDates[0] === today || sortedDates[0] === yesterday) {
          streakDays = 1;
          for (let i = 1; i < sortedDates.length; i++) {
            const prevDate = new Date(sortedDates[i - 1]);
            const currDate = new Date(sortedDates[i]);
            const diffDays = Math.round((prevDate.getTime() - currDate.getTime()) / 86400000);
            if (diffDays === 1) {
              streakDays++;
            } else {
              break;
            }
          }
        }
      }

      // Per-model projections with individual cost savings
      const modelProjections = completedJobs.map((j: any) => {
        const originalSize = getEstimatedSize(j);
        const reduction = j.sizeReduction || 0;
        const sizeSaved = Math.round(originalSize * reduction / 100);
        const latencyReduction = j.originalLatency && j.optimizedLatency
          ? Math.round((j.originalLatency - j.optimizedLatency) / j.originalLatency * 100)
          : 0;

        // Cost calculation per model
        // Storage: $0.023/GB/month
        const storageSavings = (sizeSaved / (1024 * 1024 * 1024)) * 0.023;
        // Compute: Faster inference = less GPU time
        // Assume 1000 inferences/day, $0.0001 per inference second saved
        const timeSavedPerInference = j.originalLatency && j.optimizedLatency
          ? (j.originalLatency - j.optimizedLatency) / 1000 // seconds
          : 0;
        const computeSavings = timeSavedPerInference * 1000 * 30 * 0.0001; // per month

        return {
          id: j.id,
          fileName: j.fileName,
          originalSizeMB: Math.round(originalSize / (1024 * 1024)),
          optimizedSizeMB: Math.round((originalSize - sizeSaved) / (1024 * 1024)),
          sizeReduction: reduction,
          latencyReduction,
          originalLatency: j.originalLatency || 0,
          optimizedLatency: j.optimizedLatency || 0,
          quantization: j.config?.quantization || "Unknown",
          targetDevice: j.config?.targetDevice || "Unknown",
          monthlySavings: Math.round((storageSavings + computeSavings) * 100) / 100,
          annualSavings: Math.round((storageSavings + computeSavings) * 12 * 100) / 100,
        };
      });

      // Total cost savings
      const storageSavingsPerMonth = (totalSizeSaved / (1024 * 1024 * 1024)) * 0.023;
      const computeSavingsPerMonth = avgLatencyReduction > 0
        ? (avgLatencyReduction / 100) * totalModelsOptimized * 100  // $100 per % saved per model
        : 0;
      const estimatedMonthlySavings = storageSavingsPerMonth + computeSavingsPerMonth;

      res.json({
        totalModelsOptimized,
        totalSizeSaved,
        totalSizeSavedMB: Math.round(totalSizeSaved / (1024 * 1024)),
        totalSizeSavedGB: Math.round(totalSizeSaved / (1024 * 1024 * 1024) * 100) / 100,
        avgSizeReduction: Math.round(avgSizeReduction * 10) / 10,
        avgLatencyReduction: Math.round(avgLatencyReduction * 10) / 10,
        quantizationBreakdown,
        deviceBreakdown,
        monthlyTrend,
        recentJobs,
        modelProjections,
        estimatedMonthlySavings: Math.round(estimatedMonthlySavings),
        estimatedAnnualSavings: Math.round(estimatedMonthlySavings * 12),
        streakDays,
      });
    } catch (error) {
      console.error("Error fetching insights:", error);
      res.status(500).json({ error: "Failed to fetch insights" });
    }
  });

  return httpServer;
}
